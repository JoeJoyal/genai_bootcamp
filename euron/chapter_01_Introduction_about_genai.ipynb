{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\">Agenda</font>\n",
    "\n",
    "- Comprehensive tools stack for genai application\n",
    "- Different orchestration frameworks\n",
    "- RAG (Retrieval Augmented Generation) High level concepts\n",
    "- How to choose best orchestration framework for proffessional level LLM application development."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `External API` - Travily, Serapi, weather api\n",
    "- `LLMs` - GPT, Mistral, llama, Falcon, grog api\n",
    "- `Cloud storage` - S3, GCP, Azure\n",
    "- `Orchestration Framework` - Langchain, LLamaIndex and OpenAI\n",
    "- `Vector Database` - ChromaDB, FAISS, Pinecone, Redis, MongoDB ATLAS\n",
    "- `Backend Server` - Python, Langchain, Flask, FastAPI\n",
    "- `Frontend Server` - Flask, FastAPI, Danjo, Next.js, and ReactJS.\n",
    "- `Cloud Depolyment` - AWS (Bedrock), GCP (VertexAI), Azure (OpenAI studio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"green\">Agenda</font>\n",
    "- Importance of RAG\n",
    "- Incontext learning vs RAG\n",
    "- RAG core compounds\n",
    "- Langchain: conceptual Introduction and Evolution of learning\n",
    "\n",
    "<!-- <img src=\"..\\Images\\RAG_architecture_example.png\"> -->\n",
    "\n",
    "![image info](../Images/RAG_architecture_example.png)\n",
    "\n",
    "What is a chunks?\n",
    "\n",
    "- To identify and extract specific types of information from text.\n",
    "- chunk refers to a contiguous sequence of words that represent a unit of meaning\n",
    "\n",
    "  ex: chunk_size = 500, chunk_overlap = 50\n",
    "\n",
    "![image info](../Images/chunks.png)\n",
    "\n",
    "\n",
    "#### <font color=\"green\">RAG Components</font>\n",
    "- Data Loader\n",
    "- Text Split\n",
    "- Embedding Model\n",
    "- Vector Database\n",
    "- Retriver\n",
    "- LLM Interpution\n",
    "\n",
    "\n",
    "#### <font color=\"green\">About Langchain</font>\n",
    "\n",
    "**LLM APIs**\n",
    "- OpenAI \n",
    "- LLamaIndex\n",
    "- Mistral\n",
    "- Falcon\n",
    "- So on...\n",
    "\n",
    "**Limitation of LLM**\n",
    "- Limited context windows\n",
    "- Data privacy\n",
    "- cost variances\n",
    "- No connection with 3rd parties.\n",
    "\n",
    "**Langchain**\n",
    "- Langsmith - monitoring\n",
    "- Langserver - model serving\n",
    "- LangGraph - Agent / Multi agents\n",
    "- LCEL - 2nd GenAI evolping\n",
    "\n",
    "**Langchain Evolution**\n",
    "- From LLM model to chat model.\n",
    "- From lagacy to LCEL (2nd genai evolping)\n",
    "- From langchain to langchain ecosystem.\n",
    "- From terrible documentation ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Generative AI space is the faster evolving space in software industry. No matter how hand I work the context will never be 100% up to data.`\n",
    "\n",
    "Note: But you will be reader!\n",
    "- I don't give you fish\n",
    "- I teach you, how to fish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
